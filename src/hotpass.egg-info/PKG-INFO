# Hotpass Data Refinement Pipeline

Metadata-Version: 2.4
Name: hotpass
Version: 0.1.0
Summary: Hotpass data refinement pipeline
License-Expression: LicenseRef-Proprietary
Requires-Python: <3.14,>=3.13
Description-Content-Type: text/markdown
Requires-Dist: pandas>=2.3.3
Requires-Dist: openpyxl>=3.1.5
Requires-Dist: pandera>=0.26.1
Requires-Dist: great-expectations>=1.8.0
Requires-Dist: phonenumbers>=9.0.17
Requires-Dist: rich>=14.2.0
Requires-Dist: pyyaml>=6.0.3
Provides-Extra: dev
Requires-Dist: pytest>=8.4.2; extra == "dev"
Requires-Dist: pytest-cov>=7.0.0; extra == "dev"
Requires-Dist: mypy>=1.18.2; extra == "dev"
Requires-Dist: ruff>=0.14.2; extra == "dev"
Requires-Dist: bandit>=1.8.6; extra == "dev"
Requires-Dist: black>=25.9.0; extra == "dev"
Requires-Dist: pre-commit>=4.3.0; extra == "dev"
Requires-Dist: detect-secrets>=1.5.0; extra == "dev"
Requires-Dist: types-PyYAML>=6.0.12; extra == "dev"
Provides-Extra: docs
Requires-Dist: sphinx>=8.2.3; extra == "docs"
Requires-Dist: myst-parser>=4.0.1; extra == "docs"
Requires-Dist: sphinx-autodoc-typehints>=3.5.2; extra == "docs"
Requires-Dist: furo>=2025.9.25; extra == "docs"
Requires-Dist: linkify-it-py>=2.0.3; extra == "docs"

# Hotpass Data Refinement Repository

This repository is optimized for validating, normalizing, processing, and refining multiple Excel documents into a highly-refined single source of truth. It uses Python with libraries like pandas, pandera, and great-expectations for data quality assurance.

**NEW**: Hotpass now includes comprehensive enhancements for industry-agnostic data consolidation with intelligent column mapping, advanced error handling, and multi-contact support. See [Implementation Guide](docs/implementation-guide.md) and [Gap Analysis](docs/gap-analysis.md) for details.

## Key Features

- üéØ **Industry-Agnostic**: Configurable profiles for aviation, healthcare, or any business domain
- üß† **Intelligent Column Mapping**: Automatic fuzzy matching and synonym detection
- üîç **Data Profiling**: Comprehensive statistics and quality insights
- üë• **Multi-Contact Support**: Advanced contact management per organization
- ‚ö†Ô∏è **Enhanced Error Handling**: Structured error reporting with recovery suggestions
- üé® **Rich Formatting**: Professional Excel output with conditional formatting
- üìä **Multiple Output Formats**: Export to Excel, CSV, Parquet, or JSON
- ‚úÖ **Configurable Validation**: Industry-specific thresholds and rules
- üìù **Comprehensive Logging**: Full audit trail of pipeline execution
- üí° **Quality Recommendations**: Actionable insights for data improvement
- üîß **Configuration Doctor**: Auto-diagnose and fix configuration issues
- üîÄ **Conflict Resolution**: Transparent tracking of data source conflicts

## Project Structure

- `data/`: Contains input Excel files. Run the pipeline to regenerate the refined workbook (ignored in git).
- `scripts/`: Python scripts for data processing.
- `docs/`: Additional documentation including:
  - [Implementation Guide](docs/implementation-guide.md) - How to use new features
  - [Gap Analysis](docs/gap-analysis.md) - Comprehensive enhancement roadmap
  - [Architecture Overview](docs/architecture-overview.md) - System design
  - [SSOT Field Dictionary](docs/ssot-field-dictionary.md) - Schema definitions
  - [Source Mapping](docs/source-to-target-mapping.md) - Column lineage
  - [Expectation Catalogue](docs/expectation-catalogue.md) - Validation rules
- `.github/workflows/`: GitHub Actions for automated processing.

## Setup

1. Install [uv](https://github.com/astral-sh/uv) (one-time): `curl -LsSf https://astral.sh/uv/install.sh | sh`.
2. Clone the repository and create a virtual environment: `uv venv`.
3. Synchronise dependencies (including dev/doc extras): `uv sync --extra dev --extra docs`.
4. Install pre-commit hooks: `uv run pre-commit install`.
5. Place your Excel files in the `data/` directory.
6. Run the pipeline via uv: `uv run hotpass`. Pass `--archive` to also produce a timestamped, checksum-stamped zip under `dist/` for distribution.

## CLI Usage

The CLI is distributed as a console script (`hotpass`) that exposes the full pipeline with structured logging:

```bash
# Run the pipeline with rich terminal output and enhanced features
hotpass --archive

# Emit JSON logs for automation and write the quality report to Markdown
hotpass \
  --log-format json \
  --report-path dist/quality-report.md \
  --report-format markdown

# Load defaults from a TOML configuration file and override the output path
hotpass --config config/pipeline.toml --output-path /tmp/refined.xlsx
```

### New Enhancement Features

The pipeline now includes powerful enhancement features:

- **Enhanced Formatting**: Professional Excel output with conditional formatting, auto-sized columns, and summary sheets
- **Configurable Validation**: Industry-specific validation thresholds based on your profile
- **Audit Trail**: Complete logging of all pipeline operations
- **Quality Recommendations**: Actionable suggestions for improving data quality
- **Conflict Resolution**: Transparent tracking of how data conflicts are resolved

See [Enhancement Guide](docs/enhancement-guide.md) for detailed usage examples.

Supported configuration files are JSON or TOML. CLI flags always take precedence over configuration values. When `--report-path` is provided the tool renders the `QualityReport` to Markdown by default, or HTML when `--report-format html` (or a `.html`/`.htm` extension) is used.

Structured logs can be emitted in JSON (`--log-format json`) for ingestion by automation tooling, or as rich tables (`--log-format rich`, the default) for human-friendly summaries. Each CLI run now reports detailed performance metrics ‚Äì including per-stage durations and throughput ‚Äì in both the console summary and generated quality reports so ops teams can track runtime trends.

### Performance tuning flags

Large Excel inputs can be streamed and optionally staged to parquet for reuse:

- `--excel-chunk-size <rows>` ‚Äì enable chunked reads for all source workbooks.
- `--excel-engine <name>` ‚Äì override the pandas engine (e.g. `pyxlsb` for `.xlsb` inputs).
- `--excel-stage-dir <path>` ‚Äì persist chunked reads to parquet so subsequent runs can reuse staged data.

## GitHub Actions

The repository uses ephemeral runners via GitHub Actions to automatically lint, test, and package data on pushes and pull requests. The workflow now relies on `astral-sh/setup-uv` to provision Python 3.13 and the `uv` resolver for reproducible installs:

- `uv sync --extra dev --frozen` ensures QA jobs respect the lock file.
- `uv run ruff check` and `uv run ruff format --check` enforce style and formatting.
- `uv run pytest`, `uv run mypy src scripts`, and `uv run bandit -r scripts` provide unit, type, and security coverage.
- The processing job executes `uv run hotpass --archive --dist-dir dist` before collecting artifacts.

Successful runs upload the refined workbook and zipped archive; pushes to `main` still publish the archive to the `data-artifacts` branch.

## Retrieving Packaged Artifacts

- **Local runs**: `hotpass --archive` writes the refined workbook to `data/refined_data.xlsx` and a zip archive named `refined-data-<timestamp>-<checksum>.zip` to `dist/`. The archive contains the workbook and a `SHA256SUMS` manifest so the checksum in the filename can be verified.
- **GitHub Actions artifacts**: Successful workflow runs publish two artifacts‚Äî`refined-data` (the Excel workbook) and `refined-data-archive` (the packaged zip). Download them directly from the workflow summary page.
- **Data artifacts branch**: On pushes to `main`, the `publish-artifact` job downloads the packaged zip and force-pushes it to the `data-artifacts` branch using the `DATA_ARTIFACT_PAT` secret. Consumers can fetch the branch to retrieve the latest archive without navigating workflow logs.

## Data Quality Expectations

Hotpass uses Great Expectations when available, with a manual fallback that mirrors its behaviour. Email, phone, and website columns must match their respective regex patterns for at least 85% of non-null, non-blank rows by default (configurable via `run_expectations`). Blank strings are normalised to null-equivalent values prior to validation so that optional fields do not count against match rates. Override the default `mostly` thresholds only when a specific dataset justifies a different tolerance, and record the rationale in project documentation.

## Copilot Instructions

When using GitHub Copilot for this project:

- **Validation**: Use pandera schemas to define data expectations. Example: "Create a schema for validating flight school data with columns for name, location, and contact."

- **Normalization**: Focus on cleaning data: "Normalize phone numbers and addresses in the dataframe."

- **Merging**: "Merge multiple dataframes on common keys, handling duplicates."

- **Quality Assurance**: "Add great-expectations tests for data integrity."

- **Optimization**: "Optimize the script for large datasets using chunked reading."

Ensure all code follows best practices for data processing and includes error handling.

## Documentation

Centralised documentation lives under `docs/`:

- [Architecture Overview](docs/architecture-overview.md) ‚Äì Pipeline flow, operational cadence, and privacy guardrails.
- [SSOT Field Dictionary](docs/ssot-field-dictionary.md) ‚Äì Canonical schema definitions and stewardship practices.
- [Source-to-Target Mapping](docs/source-to-target-mapping.md) ‚Äì Column-level lineage across core data providers.
- [Expectation Catalogue](docs/expectation-catalogue.md) ‚Äì Active data quality rules, thresholds, and operational notes.
- [Enhancement Guide](docs/enhancement-guide.md) ‚Äì NEW: Comprehensive guide to new features including formatting, validation, logging, and recommendations.
- [Implementation Guide](docs/implementation-guide.md) ‚Äì How to use advanced features.
- [Gap Analysis](docs/gap-analysis.md) ‚Äì Roadmap for future enhancements.
- [Quick Start](docs/quick-start.md) ‚Äì Common usage scenarios.

### Building the documentation site

```bash
uv run sphinx-build -b html docs/source docs/_build/html
open docs/_build/html/index.html  # macOS; use xdg-open on Linux
```

The build consumes Markdown via MyST and auto-generates API documentation from the `src/` package.

## Contributing

1. Make changes in a branch.
2. Test locally.
3. Create a PR; GitHub Actions will run the processing pipeline.

## Benchmarking and performance monitoring

Use the lightweight benchmarking helper to capture baseline throughput and guard against regressions:

```bash
uv run python scripts/benchmark_pipeline.py \
  --input-dir data \
  --output-path dist/benchmark.xlsx \
  --runs 5 \
  --excel-chunk-size 5000 \
  --json
```

The command runs the pipeline multiple times, aggregates average stage timings (load, aggregation, expectations, write), and emits both JSON summaries and human-readable output. The data is also available programmatically via `hotpass.benchmarks.run_benchmark` for integration into CI checks or monitoring dashboards.
