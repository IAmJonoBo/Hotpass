from __future__ import annotations

import math
import string
from collections.abc import Iterable
from pathlib import Path
from typing import Any

import pandas as pd
import pytest
from hypothesis import HealthCheck, given, settings
from hypothesis import strategies as st
from hypothesis.strategies import SearchStrategy

from hotpass.normalization import normalize_province, slugify
from hotpass.pipeline import ingestion as ingestion_module
from hotpass.pipeline.config import PipelineConfig
from hotpass.pipeline.ingestion import ingest_sources


def expect(condition: bool, message: str) -> None:
    """Shared assertion helper that mirrors project testing guidance."""

    if not condition:
        pytest.fail(message)


_BASE_COLUMNS = [
    "organization_name",
    "source_dataset",
    "source_record_id",
]

_OPTIONAL_COLUMNS = [
    "province",
    "area",
    "address",
    "category",
    "organization_type",
    "status",
    "website",
    "planes",
    "description",
    "notes",
    "last_interaction_date",
    "priority",
    "contact_names",
    "contact_roles",
    "contact_emails",
    "contact_phones",
]


def _unicode_strings() -> SearchStrategy[str]:
    """Generate messy unicode while filtering control-only payloads."""

    return st.text(
        alphabet=st.characters(
            blacklist_categories=("Cs", "Cc"),
            whitelist_categories=(
                "Ll",
                "Lu",
                "Lt",
                "Lo",
                "Nd",
                "Nl",
                "No",
                "Pc",
                "Pd",
                "Pe",
                "Pf",
                "Pi",
                "Po",
                "Ps",
                "Sm",
                "So",
                "Zs",
            ),
        ),
        min_size=1,
        max_size=30,
    )


def _row_strategy() -> SearchStrategy[dict[str, object]]:
    email_local = st.text(string.ascii_lowercase + string.digits, min_size=1, max_size=12)
    phone_digits = st.text(string.digits, min_size=9, max_size=12)
    list_strategy = st.lists(_unicode_strings(), min_size=1, max_size=3)
    return st.fixed_dictionaries(
        {
            "organization_name": _unicode_strings(),
            "source_dataset": st.sampled_from(
                ["Contact Database", "Reachout Database", "SACAA Cleaned"]
            ),
            "source_record_id": st.text(
                string.ascii_letters + string.digits, min_size=1, max_size=12
            ),
            "province": st.one_of(st.none(), _unicode_strings()),
            "area": st.one_of(st.none(), _unicode_strings()),
            "address": st.one_of(st.none(), _unicode_strings()),
            "category": st.one_of(st.none(), _unicode_strings()),
            "organization_type": st.one_of(st.none(), _unicode_strings()),
            "status": st.one_of(st.none(), _unicode_strings()),
            "website": st.one_of(
                st.none(),
                st.sampled_from(
                    [
                        "https://example.com",
                        "https://hotpass.example",
                        "http://Ã¼nicode.test",
                    ]
                ),
            ),
            "planes": st.one_of(st.none(), st.sampled_from(["0", "1", "12"])),
            "description": st.one_of(st.none(), _unicode_strings()),
            "notes": st.one_of(st.none(), _unicode_strings()),
            "last_interaction_date": st.one_of(
                st.none(),
                st.datetimes(
                    min_value=pd.Timestamp(2010, 1, 1),
                    max_value=pd.Timestamp(2040, 12, 31),
                ),
                st.dates(
                    min_value=pd.Timestamp(2010, 1, 1).date(),
                    max_value=pd.Timestamp(2040, 12, 31).date(),
                ),
                _unicode_strings(),
            ),
            "priority": st.one_of(st.none(), st.sampled_from(["High", "Medium", "Low"])),
            "contact_names": list_strategy,
            "contact_roles": list_strategy,
            "contact_emails": st.lists(
                email_local.map(lambda local: f"{local}@example.com"),
                min_size=1,
                max_size=3,
            ),
            "contact_phones": st.lists(
                phone_digits.map(lambda digits: f"+27{digits}"),
                min_size=1,
                max_size=3,
            ),
        }
    )


def _apply_duplicates(frame: pd.DataFrame, pairs: Iterable[tuple[str, str]]) -> pd.DataFrame:
    if not pairs:
        return frame
    columns = list(frame.columns)
    name_to_index = {name: idx for idx, name in enumerate(columns)}
    for extra_name, target_name in pairs:
        extra_idx = name_to_index.get(extra_name)
        target_idx = name_to_index.get(target_name)
        if extra_idx is None or target_idx is None:
            continue
        if columns[extra_idx] in _BASE_COLUMNS:
            # Never duplicate the required columns that pipeline logic depends on.
            continue
        columns[extra_idx] = columns[target_idx]
        name_to_index[columns[extra_idx]] = extra_idx
    frame = frame.copy()
    frame.columns = columns
    return frame


@given(data=st.data())
@settings(
    max_examples=25,
    deadline=None,
    suppress_health_check=[HealthCheck.function_scoped_fixture],
)
def test_ingest_sources_handles_messy_frames(
    data: Any, tmp_path: Path, monkeypatch: pytest.MonkeyPatch
) -> None:
    row_count = data.draw(st.integers(min_value=1, max_value=5))
    rows = data.draw(st.lists(_row_strategy(), min_size=row_count, max_size=row_count))

    drop_candidates = data.draw(
        st.lists(
            st.sampled_from(_OPTIONAL_COLUMNS),
            unique=True,
            max_size=len(_OPTIONAL_COLUMNS),
        )
    )

    extra_columns = data.draw(
        st.lists(
            st.text(string.ascii_lowercase, min_size=3, max_size=10).filter(
                lambda name: name not in _BASE_COLUMNS and name not in _OPTIONAL_COLUMNS
            ),
            unique=True,
            max_size=3,
        )
    )

    duplicate_pairs: list[tuple[str, str]] = []
    if extra_columns:
        duplicate_pairs = data.draw(
            st.lists(
                st.tuples(
                    st.sampled_from(extra_columns),
                    st.sampled_from(_OPTIONAL_COLUMNS),
                ),
                unique=True,
                max_size=len(extra_columns),
            )
        )

    frame = pd.DataFrame(rows)
    frame = frame.drop(columns=drop_candidates, errors="ignore")

    for column_name in extra_columns:
        frame[column_name] = [f"{column_name}_{index}" for index in range(len(frame))]

    frame = _apply_duplicates(frame, duplicate_pairs)
    frame.attrs["load_seconds"] = data.draw(
        st.floats(min_value=0.0, max_value=10.0, allow_nan=False)
    )

    input_dir = tmp_path / "inputs"
    output_dir = tmp_path / "outputs"
    input_dir.mkdir(parents=True, exist_ok=True)
    output_dir.mkdir(parents=True, exist_ok=True)

    config = PipelineConfig(
        input_dir=input_dir,
        output_path=output_dir / "refined.parquet",
    )

    def _fake_load_sources(
        _input_dir: Path, _country_code: str, _options: object
    ) -> dict[str, pd.DataFrame]:
        return {"Contact Database": frame.copy(deep=True)}

    monkeypatch.setattr(ingestion_module, "load_sources", _fake_load_sources)

    combined_one, timings_one, redactions_one = ingest_sources(config)
    combined_two, timings_two, redactions_two = ingest_sources(config)

    expect(redactions_one == [], "ingestion should not emit redaction events")
    expect(redactions_two == [], "repeated ingestion should not emit redaction events")
    expect(
        combined_one.equals(combined_two),
        "ingest_sources should be idempotent for stable inputs",
    )
    expect(timings_one == timings_two, "timings should remain stable across runs")
    expect(len(combined_one) == len(frame), "row count should align with input frames")
    expect(
        len(combined_one.columns) == len(set(combined_one.columns)),
        "column headers must be deduplicated",
    )

    for original, slug in zip(
        frame["organization_name"], combined_one["organization_slug"], strict=True
    ):
        expect(slug == slugify(original), "slugified organization name should match helper")
        expect(slug is None or slug.isascii(), "slug outputs should remain ASCII-safe")

    if "province" in frame.columns:
        province_source = frame["province"]
        if isinstance(province_source, pd.DataFrame):
            province_source = province_source.iloc[:, 0]
        for original, normalized in zip(province_source, combined_one["province"], strict=True):
            expect(
                normalized == normalize_province(original),
                "province normalization must align with helper",
            )
    else:
        expect(
            "province" in combined_one.columns,
            "missing provinces should be reintroduced as empty column",
        )

    expect(
        math.isclose(sum(timings_one.values()), sum(timings_two.values())),
        "timing sums should remain stable",
    )
